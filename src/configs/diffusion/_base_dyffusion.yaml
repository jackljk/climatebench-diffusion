# @package _global_

defaults:
  - _base.yaml
  - _self_

diffusion:
  timesteps: ${datamodule.horizon}  # Do not change, it is automatically inferred by DYffusion
  # How to condition the forecaster network. Options: "none", "data", "data+noise"
  # If "data", condition on the input data (i.e., the initial conditions at time t0)
  # If "data+noise", condition on a linear interpolation between the input data and a noise sample
  # If "none", do not condition the forecaster network. The only input will be the output of the interpolator
  forward_conditioning: "data"

  # Using auxiliary diffusion steps (k>0 in the paper)
  # The following parameters are only used if additional_interpolation_steps or additional_interpolation_steps_factor > 0
  schedule: 'before_t1_only'     # If 'before_t1_only', all auxiliary diffusion steps are added before t1
  additional_interpolation_steps: 0   # k, how many additional diffusion steps to add. Only used if schedule='before_t1_only'
  additional_interpolation_steps_factor: 0  # only use if schedule='linear'
  interpolate_before_t1: True   # Whether to interpolate before t1 too. Must be true if schedule='before_t1_only'

  # Time encoding refers to the way the time is encoded for the forecaster network for a given diffusion step.
  time_encoding: "dynamics"   # Options: "dynamics", "discrete". Recommended: "dynamics", i.e. use actual timestep

  # Enabling stochastic dropout in the interpolator is strongly recommended for better performance
  enable_interpolator_dropout: True   # Keep True!
  enable_predict_last_dropout: False
  interpolator_use_ema: False
  
  # ---- Sampling related parameters:
  # Sampling algorithm. Options: 'cold', 'naive'. Strongly recommended: 'cold'
  sampling_type: 'cold'
  # Accelerate sampling when k > 0, by using fewer diffusion steps by skipping some auxiliary diffusion steps
  sampling_schedule: null   # E.g. set to "every2nd" to skip every second auxiliary diffusion step. Only used if k > 0

  # Whether to refine the intermediate interpolor predictions by re-running the interpolator (line 6 in Algorithm 1)
  # It is recommended to set this to False during training
  # At validation time you may set it to True to see if it improves the results
  #  python run.py mode=test diffusion.refine_intermediate_predictions=True logger.wandb.id=???
  use_same_dropout_state_for_sampling: False
  use_cold_sampling_for_intermediate_steps: True
  use_cold_sampling_for_last_step: True
  use_cold_sampling_for_init_of_ar_step: null
  #
  log_every_t: null